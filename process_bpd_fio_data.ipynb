{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_from_api_url(url: str) -> pd.DataFrame:\n",
    "    \"\"\"Convert data.boston.gov's JSON API response into a dataframe.\"\"\"\n",
    "    json = requests.get(url).json()\n",
    "    records = json[\"result\"][\"records\"]\n",
    "    df = pd.DataFrame(records).astype(str)\n",
    "    # drop \"_id\" column, added by CKAN (I think?)\n",
    "    df.drop(columns=[\"_id\"], inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_url(resource_id: str) -> str:\n",
    "    return f\"https://data.boston.gov/api/3/action/datastore_search?resource_id={resource_id}&limit=100000\"\n",
    "\n",
    "\n",
    "# List table URLs by order of year: 2015 (June onwards), 2016, 2017, 2018, 2019\n",
    "contact_urls = [\n",
    "    make_url(\"8119eb8d-6ee8-412d-a45d-53c367a98cea\"),\n",
    "    make_url(\"35f3fb8f-4a01-4242-9758-f664e7ead125\"),\n",
    "    make_url(\"c72b9288-2658-4e6a-9686-ffdcacb585e7\"),\n",
    "    make_url(\"ee4f1175-54b6-4d06-bceb-26d349118e25\"),\n",
    "    make_url(\"35cfa498-cb10-43da-b8b2-948a66e48f26\"),\n",
    "    make_url(\"03f33240-47c1-46f2-87ae-bcdabec092ad\"),\n",
    "]\n",
    "people_urls = [\n",
    "    make_url(\"34453828-67ca-45f1-a31d-526b11ca49f4\"),\n",
    "    make_url(\"ebb9c51c-6e9a-40a4-94d0-895de9bf47ad\"),\n",
    "    make_url(\"f18a0632-46ea-4032-9749-f5b50cf7b865\"),\n",
    "    make_url(\"aa46b3ad-1526-4551-9f0f-6dbdfbb429c0\"),\n",
    "    make_url(\"b102d3a4-8b44-443e-bc09-00c44974c3b1\"),\n",
    "    make_url(\"2d29a168-534b-47c4-977a-b8f4aaf2ea8c\"),\n",
    "]\n",
    "\n",
    "# Load, join, and concatenate all dataframes\n",
    "contact_df = pd.concat([df_from_api_url(url) for url in contact_urls], sort=False)\n",
    "people_df = pd.concat([df_from_api_url(url) for url in people_urls], sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true,
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "df_transforms = []\n",
    "\n",
    "\n",
    "def register_transform(f: callable) -> callable:\n",
    "    df_transforms.append(f)\n",
    "    return f\n",
    "\n",
    "\n",
    "def apply_transforms(\n",
    "    contact_df: pd.DataFrame, people_df: pd.DataFrame\n",
    ") -> (pd.DataFrame, pd.DataFrame):\n",
    "    for t in df_transforms:\n",
    "        contact_df, people_df = t(contact_df, people_df)\n",
    "    return contact_df, people_df\n",
    "\n",
    "\n",
    "@register_transform\n",
    "def deduplicate_fc_num(contact_df, people_df):\n",
    "    \"\"\"\n",
    "    Around 25 records in the **contacts** table contain duplicate `fc_num`s, which is supposed to be a unique \n",
    "    identifier for a given stop. Manual inspection of the data suggests this occurs when officers update the `basis`\n",
    "    field for a contact report after initially entering it, but this is just my off-the-cuff guess. Based on \n",
    "    this guess, remove duplicates by taking that last record with a duplicated `fc_num` as the intended entry for\n",
    "    that contact.\n",
    "    \"\"\"\n",
    "    return contact_df.drop_duplicates(subset=\"fc_num\", keep=\"last\"), people_df\n",
    "\n",
    "\n",
    "@register_transform\n",
    "def lowercase_everything(contact_df, people_df):\n",
    "    \"\"\"Convert all string columns to lowercase except `fc_num`, since capitalization is inconsistent throughout.\"\"\"\n",
    "\n",
    "    def to_lower(col):\n",
    "        if col.name == \"fc_num\":\n",
    "            return col\n",
    "        return col.str.lower()\n",
    "\n",
    "    contact_df = contact_df.apply(to_lower)\n",
    "    people_df = people_df.apply(to_lower)\n",
    "\n",
    "    return contact_df, people_df\n",
    "\n",
    "\n",
    "@register_transform\n",
    "def rename_frisk_columns(contact_df, people_df):\n",
    "    \"\"\"\n",
    "    Combine the `frisked` and `searchperson` columns from the **contacts** table into one column called `fc_involved_frisk_or_search`,\n",
    "    and disambiguate it from a related column in the **people** table by renaming the `frisk/search` column to `person_frisked_or_searched`.\n",
    "    As noted on data.boston.gov, the `frisked` and `searchperson` columns (from the \"New RMS\" data system) indicate whether any of the \n",
    "    individuals stopped in a given field contact was frisked, whereas `frisk/search` (from the \"Mark43\" data system) indicates whether\n",
    "    a particular person involved in a field contact was frisked.\n",
    "    \"\"\"\n",
    "    combine_fields = (\n",
    "        lambda row: \"y\"\n",
    "        if row.frisked == \"y\" or row.searchperson == \"y\"\n",
    "        else row.frisked\n",
    "    )\n",
    "    combined_values = contact_df[[\"frisked\", \"searchperson\"]].apply(\n",
    "        combine_fields, axis=1\n",
    "    )\n",
    "    contact_df = contact_df.assign(fc_involved_frisk_or_search=combined_values)\n",
    "    contact_df = contact_df.drop(columns=[\"frisked\", \"searchperson\"])\n",
    "\n",
    "    people_df = people_df.rename(columns={\"frisk/search\": \"person_frisked_or_searched\"})\n",
    "    people_df.person_frisked_or_searched = people_df.person_frisked_or_searched.replace(\n",
    "        {\"0\": \"n\", \"1\": \"y\"}\n",
    "    )\n",
    "\n",
    "    # The newer system doesn't provide contact-level frisk info,\n",
    "    # but we can infer whether a contact involved a frisk/search based on\n",
    "    # whether the people involved in that contact were frisked.\n",
    "    contacts_with_no_data = contact_df.fc_involved_frisk_or_search.isnull()\n",
    "    contact_involved_frisk = people_df.groupby(\"fc_num\").apply(\n",
    "        lambda g: \"y\"\n",
    "        if (g.person_frisked_or_searched == \"y\").any()\n",
    "        else \"n\"\n",
    "        if (g.person_frisked_or_searched == \"n\").all()\n",
    "        else np.nan\n",
    "    )\n",
    "    contact_df.loc[contacts_with_no_data, \"fc_involved_frisk_or_search\"] = contact_df[\n",
    "        contacts_with_no_data\n",
    "    ].apply(lambda row: contact_involved_frisk.loc[row.fc_num], axis=1)\n",
    "\n",
    "    return contact_df, people_df\n",
    "\n",
    "\n",
    "@register_transform\n",
    "def vehicle_info_cleanup(contact_df, people_df):\n",
    "    \"\"\"\n",
    "    Clean up and reconcile vehicle-related field values from the **contacts** table. Across the different data systems, different text values\n",
    "    were used to represent identical or overlapping concepts (e.g. \"LT. BLUE\" and \"light blue\", \"Suv (sport Utility Vehicle)\" \n",
    "    and \"SUV or Utility Van\"). Also, drop the `vehicle_style` column, which is basically a noisier version of the `vehicle_type` column.\n",
    "    \"\"\"\n",
    "    contact_df.vehicle_type = contact_df.vehicle_type.replace(\n",
    "        {\n",
    "            \"scooter\": \"motorcycle or scooter\",\n",
    "            \"cargo van\": \"suv or utility van\",\n",
    "            \"suv (sport utility vehicle)\": \"suv or utility van\",\n",
    "            \"passenger van\": \"bus or passenger van\",\n",
    "            \"bus/passenger van\": \"bus or passenger van\",\n",
    "            \"passenger car/ automobile\": \"passenger car\",\n",
    "        }\n",
    "    )\n",
    "    contact_df.vehicle_color = contact_df.vehicle_color.str.strip().replace(\n",
    "        {\n",
    "            \"bla\": \"black\",\n",
    "            \"gra\": \"gray\",\n",
    "            \"gre\": \"green\",\n",
    "            \"lt. green\": \"light green\",\n",
    "            \"lt. blue\": \"light blue\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    def fix_year(year: str) -> str:\n",
    "        if not year or year in (\"none\", \"null\"):\n",
    "            return year\n",
    "        num_year = float(year)\n",
    "        if np.isnan(num_year):\n",
    "            return year\n",
    "        if num_year > 1900:\n",
    "            return year\n",
    "        if num_year > 99:\n",
    "            # Exclude definite typos\n",
    "            return \"null\"\n",
    "        if num_year < 10:\n",
    "            return f\"200{year}\"\n",
    "        if num_year < 21:\n",
    "            return f\"20{year}\"\n",
    "        else:\n",
    "            return f\"19{year}\"\n",
    "\n",
    "    contact_df.vehicle_year = contact_df.vehicle_year.apply(fix_year)\n",
    "\n",
    "    return contact_df, people_df\n",
    "\n",
    "\n",
    "@register_transform\n",
    "def contact_date_to_dt(contact_df, people_df):\n",
    "    \"\"\"\n",
    "    Convert the `contact_date` column in the **contacts** table to datetime, \n",
    "    and drop a duplicate column from the **people** table.\n",
    "    \"\"\"\n",
    "    # One record in the 2015 dataset's contact_date appears to have gotten corrupted in some way,\n",
    "    # so just set that record's contact_date to \"2015\"\n",
    "    weird_date = '0217-04-17t12:30:00'\n",
    "    contact_df = contact_df[contact_df.contact_date != weird_date]\n",
    "    people_df = people_df[people_df.contact_date != weird_date]\n",
    "\n",
    "    contact_df.contact_date = pd.to_datetime(contact_df.contact_date)\n",
    "    \n",
    "    # Certain contact date records for 2015 have years less than 2015 in their contact date.\n",
    "    # This shouldn't be so.\n",
    "    bad_year = contact_df.contact_date.dt.year.astype(int) < 2015\n",
    "    contact_df.loc[bad_year, 'contact_date'] = contact_df.contact_date[bad_year] + pd.offsets.DateOffset(year=2015)\n",
    "    \n",
    "    people_df = people_df.drop(columns=[\"contact_date\"])\n",
    "\n",
    "    return contact_df, people_df\n",
    "\n",
    "\n",
    "@register_transform\n",
    "def clean_officer_and_supervisor_names(contact_df, people_df):\n",
    "    \"\"\"\n",
    "    Remove extra whitespace from the `contact_officer_name` column in the **contacts** table \n",
    "    to reduce accidental duplication. Also, put officer's first names first.\n",
    "    \"\"\"\n",
    "    observed_names = {}\n",
    "\n",
    "    def clean_name(id: str, name: str) -> str:\n",
    "        if id in observed_names:\n",
    "            return observed_names[id]\n",
    "\n",
    "        name = name.replace(\".\", \"\")\n",
    "        \n",
    "        if name.strip() == \",\":\n",
    "            return np.nan\n",
    "        \n",
    "        split_name = [part.strip() for part in name.split(\",\")]\n",
    "        clean_name = None\n",
    "        if len(split_name) == 1:\n",
    "            clean_name = split_name[0]\n",
    "        elif len(split_name) <= 3:\n",
    "            last_part, *first_part = split_name\n",
    "            clean_name = f'{\" \".join(first_part)} {last_part}'\n",
    "\n",
    "        if clean_name is None:\n",
    "            raise Exception(f\"Encountered name with unexpected structure: {name}\")\n",
    "\n",
    "        observed_names[id] = clean_name\n",
    "        return clean_name\n",
    "\n",
    "    contact_df.supervisor_name = contact_df.apply(\n",
    "        lambda row: clean_name(row.supervisor, row.supervisor_name), axis=1\n",
    "    )\n",
    "    contact_df.contact_officer_name = contact_df.apply(\n",
    "        lambda row: clean_name(row.contact_officer, row.contact_officer_name), axis=1\n",
    "    )\n",
    "    return contact_df, people_df\n",
    "\n",
    "\n",
    "@register_transform\n",
    "def combine_contact_reason_and_narrative(contact_df, people_df):\n",
    "    \"\"\"\n",
    "    Merge the **contacts** table's `contact_reason` column into the `narrative` column. `contact_reason` serves the same purpose as `narrative`,\n",
    "    just for the older system. Uppercase both columns for consistency.\n",
    "    \"\"\"\n",
    "    empty_narrative = contact_df.narrative.isnull()\n",
    "    contact_df.loc[empty_narrative, \"narrative\"] = contact_df[\n",
    "        empty_narrative\n",
    "    ].contact_reason\n",
    "    contact_df = contact_df.drop(columns=[\"contact_reason\"])\n",
    "    return contact_df, people_df\n",
    "\n",
    "\n",
    "@register_transform\n",
    "def clean_up_city(contact_df, people_df):\n",
    "    \"\"\"\n",
    "    There are lots of typos and inconsistencies in the **contacts** table's `city` column, so fix them.\n",
    "    Warning: current solution is not robust to *new* typos, should the data change.\n",
    "    \"\"\"\n",
    "    BOSTON = \"boston\"\n",
    "    SOUTHIE = \"south boston\"\n",
    "    DORCHESTER = \"dorchester\"\n",
    "    CHARLESTOWN = \"charlestown\"\n",
    "    JP = \"jamaica plain\"\n",
    "    EASTIE = \"east boston\"\n",
    "    MATTAPAN = \"mattapan\"\n",
    "    ROXBURY = \"roxbury\"\n",
    "    HYDEPARK = \"hyde park\"\n",
    "    contact_df.city = contact_df.city.replace(\n",
    "        {\n",
    "            \"bstn\": BOSTON,\n",
    "            \"so.boston\": SOUTHIE,\n",
    "            \"dorcchester\": DORCHESTER,\n",
    "            \"chaarlestown\": CHARLESTOWN,\n",
    "            \"jp\": JP,\n",
    "            \"east bos\": EASTIE,\n",
    "            \"chalrestown\": CHARLESTOWN,\n",
    "            \"east bostn\": EASTIE,\n",
    "            \"dor\": DORCHESTER,\n",
    "            \"mt\": MATTAPAN,\n",
    "            \"s boston\": SOUTHIE,\n",
    "            \"dorchster\": DORCHESTER,\n",
    "            \"bst\": BOSTON,\n",
    "            \"s bstn\": SOUTHIE,\n",
    "            \"dorchesterr\": DORCHESTER,\n",
    "            \"jamaiica plain\": JP,\n",
    "            \"roxbury ma\": ROXBURY,\n",
    "            \"so boston\": SOUTHIE,\n",
    "            \"e. boston\": EASTIE,\n",
    "            \"jamaica\": JP,\n",
    "            \"ddorchester\": DORCHESTER,\n",
    "            \"mattpan\": MATTAPAN,\n",
    "            \"jamaicia\": JP,\n",
    "            \"s. boston\": SOUTHIE,\n",
    "            \"hp\": HYDEPARK,\n",
    "            \"dorchest\": DORCHESTER,\n",
    "            \"sbos\": SOUTHIE,\n",
    "            \"rox\": ROXBURY,\n",
    "            \"charlestwon\": CHARLESTOWN,\n",
    "            \"jamacia plain\": JP,\n",
    "            \"robury\": ROXBURY,\n",
    "            \"btsn\": BOSTON,\n",
    "            \"sommerville\": \"somerville\",\n",
    "            \"jamaicia plain\": JP,\n",
    "            \"s.boston\": SOUTHIE,\n",
    "            \"dor.\": DORCHESTER,\n",
    "            \"e boston\": EASTIE,\n",
    "            \"e.boston\": EASTIE,\n",
    "            \"bostob\": BOSTON,\n",
    "            \"roslindlae\": \"ROSLINDALE\",\n",
    "            \"bsnt\": BOSTON,\n",
    "            \"bstna\": BOSTON,\n",
    "            \"bston\": BOSTON,\n",
    "            \"jamaiaca plain\": JP,\n",
    "            \"so. boston\": SOUTHIE,\n",
    "            \"unkown\": \"unknown\",\n",
    "        }\n",
    "    )\n",
    "    return contact_df, people_df\n",
    "\n",
    "\n",
    "@register_transform\n",
    "def clean_up_age(contact_df, people_df):\n",
    "    \"\"\"Remove implausibly high values in the **people** table's `age` column. Also, convert string values to float.\"\"\"\n",
    "    people_df.age = people_df.age.apply(\n",
    "        lambda age: np.nan if len(age) > 2 or age == \"\" else np.float(age)\n",
    "    )\n",
    "    return contact_df, people_df\n",
    "\n",
    "\n",
    "@register_transform\n",
    "def combine_skin_tone_and_complexion(contact_df, people_df):\n",
    "    \"\"\"Merge the **people** table's older `complexion` column into the newer `skin_tone` column.\"\"\"\n",
    "    empty_skin_tone = people_df.skin_tone.isnull()\n",
    "    people_df.loc[empty_skin_tone, \"skin_tone\"] = people_df.complexion[empty_skin_tone]\n",
    "    people_df.skin_tone = people_df.skin_tone\n",
    "    people_df = people_df.drop(columns=[\"complexion\"])\n",
    "    return contact_df, people_df\n",
    "\n",
    "\n",
    "@register_transform\n",
    "def drop_deceased_column(contact_df, people_df):\n",
    "    \"\"\"\n",
    "    Drop the `deceased` column from the **people** table, since it doesn't exist in the older system, and since no one \n",
    "    is marked deceased in this dataset.\n",
    "    \"\"\"\n",
    "    people_df = people_df.drop(columns=[\"deceased\"])\n",
    "    return contact_df, people_df\n",
    "\n",
    "\n",
    "@register_transform\n",
    "def reconcile_hair_style(contact_df, people_df):\n",
    "    \"\"\"Reconcile the **people** table's `hair_style` values that seem to mean the same thing.\"\"\"\n",
    "    people_df.hair_style = people_df.hair_style.replace(\n",
    "        {\n",
    "            \"receding / thin\": \"receding or thin\",\n",
    "            \"receding or slightly receding\": \"receding or thin\",\n",
    "            \"bald\": \"bald or balding\",\n",
    "            \"braids\": \"braided\",\n",
    "            \"wig/hair piece\": \"wig or hair piece\",\n",
    "        }\n",
    "    )\n",
    "    return contact_df, people_df\n",
    "\n",
    "\n",
    "@register_transform\n",
    "def reconcile_race(contact_df, people_df):\n",
    "    \"\"\"Reconcile **people** table `race` values that seem to mean the same thing.\"\"\"\n",
    "    people_df.race = people_df.race.replace(\n",
    "        {\"american indian or alaskan native\": \"native american / alaskan native\"}\n",
    "    )\n",
    "    return contact_df, people_df\n",
    "\n",
    "\n",
    "@register_transform\n",
    "def clean_up_state(contact_df, people_df):\n",
    "    \"\"\"Fix recurrent typo in the **contacts** table `state` column.\"\"\"\n",
    "    contact_df.state = contact_df.state.replace({\"MX\": \"MA\"})\n",
    "    return contact_df, people_df\n",
    "\n",
    "\n",
    "@register_transform\n",
    "def reconcile_stop_duration(contact_df, people_df):\n",
    "    \"\"\"Bucket **contacts** table `stop_duration` column values listed as minutes and remove likely typos.\"\"\"\n",
    "\n",
    "    def bucket_stop_duration(d: str) -> str:\n",
    "        try:\n",
    "            d = int(d)\n",
    "        except:\n",
    "            return d\n",
    "\n",
    "        # A stop longer than 8 hours seems really implausible\n",
    "        if d > 500:\n",
    "            return \"null\"\n",
    "        if d < 5:\n",
    "            return \"less than five minutes\"\n",
    "        if d < 10:\n",
    "            return \"five to ten minutes\"\n",
    "        if d < 15:\n",
    "            return \"ten to fifteen minutes\"\n",
    "        if d < 20:\n",
    "            return \"fifteen to twenty minutes\"\n",
    "        if d < 25:\n",
    "            return \"twenty to twenty-five minutes\"\n",
    "        if d < 30:\n",
    "            return \"twenty-five to thirty minutes\"\n",
    "        if d < 45:\n",
    "            return \"thirty to forty-five minutes\"\n",
    "        if d < 60:\n",
    "            return \"forty-five to sixty minutes\"\n",
    "        if d < 120:\n",
    "            return \"one to two hours\"\n",
    "        return \"longer than two hours\"\n",
    "\n",
    "    contact_df.stop_duration = contact_df.stop_duration.apply(bucket_stop_duration)\n",
    "    return contact_df, people_df\n",
    "\n",
    "\n",
    "@register_transform\n",
    "def clean_location_info(contact_df, people_df):\n",
    "    \"\"\"\n",
    "    Combine the **contacts** table's `street` and `streetaddr` columns into single `street` column, \n",
    "    drop `streedaddr`, and trim extra digits from `zip`.\n",
    "    \"\"\"\n",
    "    empty_street = contact_df.street.isnull()\n",
    "    contact_df.loc[empty_street, \"street\"] = contact_df.streetaddr[empty_street]\n",
    "    contact_df.street = contact_df.street.apply(lambda s: s.replace(\"&\", \"/\"))\n",
    "\n",
    "    contact_df = contact_df.drop(columns=[\"streetaddr\"])\n",
    "\n",
    "    contact_df.zip = contact_df.zip.apply(lambda zip: zip.split(\"-\")[0])\n",
    "\n",
    "    return contact_df, people_df\n",
    "\n",
    "\n",
    "@register_transform\n",
    "def sex_to_gender(contact_df, people_df):\n",
    "    \"\"\"\n",
    "    The `sex` column in the **people** table might more accurately be called `gender`. Also, tweak column values accordingly.\n",
    "    \"\"\"\n",
    "    people_df = people_df.rename(columns={\"sex\": \"gender\"})\n",
    "    people_df.gender = people_df.gender.replace(\n",
    "        {\n",
    "            \"male\": \"man\",\n",
    "            \"female\": \"woman\",\n",
    "            \"transgender male to female\": \"transgender woman\",\n",
    "            \"transgender female to male\": \"transgender man\",\n",
    "        }\n",
    "    )\n",
    "    return contact_df, people_df\n",
    "\n",
    "@register_transform\n",
    "def add_year_column(contact_df, people_df):\n",
    "    \"\"\"\n",
    "    Add a `year` column derived from the `contact_date` column.\n",
    "    \"\"\"\n",
    "    contact_df = contact_df.assign(year=contact_df.contact_date.dt.year.astype(str))\n",
    "    return contact_df, people_df\n",
    "\n",
    "@register_transform\n",
    "def clean_empty_values(contact_df, people_df):\n",
    "    \"\"\"\n",
    "    The values `''`, `'NULL'`, and `'None'` are used to signify no value was entered in various fields across the dataset.\n",
    "    Replace these values with `np.nan`.\n",
    "    \"\"\"\n",
    "    replacements = {\"\": np.nan, \"null\": np.nan, \"none\": np.nan}\n",
    "    contact_df = contact_df.replace(replacements)\n",
    "    people_df = people_df.replace(replacements)\n",
    "    return contact_df, people_df\n",
    "\n",
    "clean_contact_df, clean_people_df = apply_transforms(contact_df, people_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the cleaned data as CSVs and as a nested JSON blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_contact_df.to_csv(\"fio_contacts.csv\", index=False)\n",
    "clean_people_df.to_csv(\"fio_people.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_camel_case(snake_str):\n",
    "    components = snake_str.split(\"_\")\n",
    "    return components[0] + \"\".join(x.title() for x in components[1:])\n",
    "\n",
    "\n",
    "json_contact_df, json_people_df = clean_contact_df.copy(), clean_people_df.copy()\n",
    "json_contact_df.columns = json_contact_df.columns.map(to_camel_case)\n",
    "json_people_df.columns = json_people_df.columns.map(to_camel_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_dicts = json_people_df.groupby(\"fcNum\").apply(lambda g: g.to_dict(\"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "json_contact_df = json_contact_df.set_index(\"fcNum\")\n",
    "json_contact_df[\"fcNum\"] = json_contact_df.index\n",
    "json_contact_df[\"people\"] = json_contact_df.index.map(\n",
    "    lambda fc_num: people_dicts.loc[fc_num]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_included_values(people, key):\n",
    "    values = set()\n",
    "    for person in people:\n",
    "        value = person[key]\n",
    "        if pd.isnull(value):\n",
    "            values.add('[not reported]')\n",
    "        else:\n",
    "            values.add(value)\n",
    "    return '|'.join(values)\n",
    "\n",
    "def add_people_values(row):\n",
    "    people = row['people']\n",
    "    row['includedGenders'] = get_included_values(people, 'gender')\n",
    "    row['includedRaces'] = get_included_values(people, 'race')\n",
    "    row['includedEthnicities'] = get_included_values(people, 'ethnicity')\n",
    "    return row\n",
    "    \n",
    "json_contact_df = json_contact_df.apply(add_people_values, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"nested_fio_data.json\", \"w\") as json_file:\n",
    "    json_contact_df.to_json(json_file, orient=\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto-populate the README with changes applied to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "readme_changelist = \"\\n\".join([f\"- {f.__doc__.strip()}\" for f in df_transforms])\n",
    "\n",
    "with open(\"README.md\", \"r\") as readme:\n",
    "    readme_body = readme.read()\n",
    "    readme_trunc_body, _ = readme_body.split(\"## Data-cleaning operations\")\n",
    "\n",
    "with open(\"README.md\", \"w\") as readme:\n",
    "    readme.write(\n",
    "        f\"{readme_trunc_body}\\n## Data-cleaning operations\\n{readme_changelist}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
