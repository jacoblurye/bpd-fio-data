{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"import requests\\nimport numpy as np\\nimport pandas as pd\\n\\n%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"import requests\\nimport numpy as np\\nimport pandas as pd\\n\\n%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"def df_from_api_url(url: str) -> pd.DataFrame:\\n    \\\"\\\"\\\"Convert data.boston.gov's JSON API response into a dataframe.\\\"\\\"\\\"\\n    json = requests.get(url).json()\\n    records = json[\\\"result\\\"][\\\"records\\\"]\\n    df = pd.DataFrame(records).astype(str)\\n    # drop \\\"_id\\\" column, added by CKAN (I think?)\\n    df.drop(columns=[\\\"_id\\\"], inplace=True)\\n    return df\\n\\n\\ndef make_url(resource_id: str) -> str:\\n    return f\\\"https://data.boston.gov/api/3/action/datastore_search?resource_id={resource_id}&limit=100000\\\"\\n\\n\\n# List table URLs by order of year: 2016, 2017, 2018, 2019\\ncontact_urls = [\\n    make_url(\\\"35f3fb8f-4a01-4242-9758-f664e7ead125\\\"),\\n    make_url(\\\"c72b9288-2658-4e6a-9686-ffdcacb585e7\\\"),\\n    make_url(\\\"ee4f1175-54b6-4d06-bceb-26d349118e25\\\"),\\n    make_url(\\\"35cfa498-cb10-43da-b8b2-948a66e48f26\\\"),\\n    make_url(\\\"03f33240-47c1-46f2-87ae-bcdabec092ad\\\"),\\n]\\npeople_urls = [\\n    make_url(\\\"ebb9c51c-6e9a-40a4-94d0-895de9bf47ad\\\"),\\n    make_url(\\\"f18a0632-46ea-4032-9749-f5b50cf7b865\\\"),\\n    make_url(\\\"aa46b3ad-1526-4551-9f0f-6dbdfbb429c0\\\"),\\n    make_url(\\\"b102d3a4-8b44-443e-bc09-00c44974c3b1\\\"),\\n    make_url(\\\"2d29a168-534b-47c4-977a-b8f4aaf2ea8c\\\"),\\n]\\n\\n# Load, join, and concatenate all dataframes\\ncontact_df = pd.concat([df_from_api_url(url) for url in contact_urls], sort=False)\\npeople_df = pd.concat([df_from_api_url(url) for url in people_urls], sort=False)\";\n",
       "                var nbb_formatted_code = \"def df_from_api_url(url: str) -> pd.DataFrame:\\n    \\\"\\\"\\\"Convert data.boston.gov's JSON API response into a dataframe.\\\"\\\"\\\"\\n    json = requests.get(url).json()\\n    records = json[\\\"result\\\"][\\\"records\\\"]\\n    df = pd.DataFrame(records).astype(str)\\n    # drop \\\"_id\\\" column, added by CKAN (I think?)\\n    df.drop(columns=[\\\"_id\\\"], inplace=True)\\n    return df\\n\\n\\ndef make_url(resource_id: str) -> str:\\n    return f\\\"https://data.boston.gov/api/3/action/datastore_search?resource_id={resource_id}&limit=100000\\\"\\n\\n\\n# List table URLs by order of year: 2016, 2017, 2018, 2019\\ncontact_urls = [\\n    make_url(\\\"35f3fb8f-4a01-4242-9758-f664e7ead125\\\"),\\n    make_url(\\\"c72b9288-2658-4e6a-9686-ffdcacb585e7\\\"),\\n    make_url(\\\"ee4f1175-54b6-4d06-bceb-26d349118e25\\\"),\\n    make_url(\\\"35cfa498-cb10-43da-b8b2-948a66e48f26\\\"),\\n    make_url(\\\"03f33240-47c1-46f2-87ae-bcdabec092ad\\\"),\\n]\\npeople_urls = [\\n    make_url(\\\"ebb9c51c-6e9a-40a4-94d0-895de9bf47ad\\\"),\\n    make_url(\\\"f18a0632-46ea-4032-9749-f5b50cf7b865\\\"),\\n    make_url(\\\"aa46b3ad-1526-4551-9f0f-6dbdfbb429c0\\\"),\\n    make_url(\\\"b102d3a4-8b44-443e-bc09-00c44974c3b1\\\"),\\n    make_url(\\\"2d29a168-534b-47c4-977a-b8f4aaf2ea8c\\\"),\\n]\\n\\n# Load, join, and concatenate all dataframes\\ncontact_df = pd.concat([df_from_api_url(url) for url in contact_urls], sort=False)\\npeople_df = pd.concat([df_from_api_url(url) for url in people_urls], sort=False)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def df_from_api_url(url: str) -> pd.DataFrame:\n",
    "    \"\"\"Convert data.boston.gov's JSON API response into a dataframe.\"\"\"\n",
    "    json = requests.get(url).json()\n",
    "    records = json[\"result\"][\"records\"]\n",
    "    df = pd.DataFrame(records).astype(str)\n",
    "    # drop \"_id\" column, added by CKAN (I think?)\n",
    "    df.drop(columns=[\"_id\"], inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_url(resource_id: str) -> str:\n",
    "    return f\"https://data.boston.gov/api/3/action/datastore_search?resource_id={resource_id}&limit=100000\"\n",
    "\n",
    "\n",
    "# List table URLs by order of year: 2016, 2017, 2018, 2019\n",
    "contact_urls = [\n",
    "    make_url(\"35f3fb8f-4a01-4242-9758-f664e7ead125\"),\n",
    "    make_url(\"c72b9288-2658-4e6a-9686-ffdcacb585e7\"),\n",
    "    make_url(\"ee4f1175-54b6-4d06-bceb-26d349118e25\"),\n",
    "    make_url(\"35cfa498-cb10-43da-b8b2-948a66e48f26\"),\n",
    "    make_url(\"03f33240-47c1-46f2-87ae-bcdabec092ad\"),\n",
    "]\n",
    "people_urls = [\n",
    "    make_url(\"ebb9c51c-6e9a-40a4-94d0-895de9bf47ad\"),\n",
    "    make_url(\"f18a0632-46ea-4032-9749-f5b50cf7b865\"),\n",
    "    make_url(\"aa46b3ad-1526-4551-9f0f-6dbdfbb429c0\"),\n",
    "    make_url(\"b102d3a4-8b44-443e-bc09-00c44974c3b1\"),\n",
    "    make_url(\"2d29a168-534b-47c4-977a-b8f4aaf2ea8c\"),\n",
    "]\n",
    "\n",
    "# Load, join, and concatenate all dataframes\n",
    "contact_df = pd.concat([df_from_api_url(url) for url in contact_urls], sort=False)\n",
    "people_df = pd.concat([df_from_api_url(url) for url in people_urls], sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false,
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"df_transforms = []\\n\\n\\ndef register_transform(f: callable) -> callable:\\n    df_transforms.append(f)\\n    return f\\n\\n\\ndef apply_transforms(\\n    contact_df: pd.DataFrame, people_df: pd.DataFrame\\n) -> (pd.DataFrame, pd.DataFrame):\\n    for t in df_transforms:\\n        contact_df, people_df = t(contact_df, people_df)\\n    return contact_df, people_df\\n\\n\\n@register_transform\\ndef deduplicate_fc_num(contact_df, people_df):\\n    \\\"\\\"\\\"\\n    Around 25 records in the **contacts** table contain duplicate `fc_num`s, which is supposed to be a unique \\n    identifier for a given stop. Manual inspection of the data suggests this occurs when officers update the `basis`\\n    field for a contact report after initially entering it, but this is just my off-the-cuff guess. Based on \\n    this guess, remove duplicates by taking that last record with a duplicated `fc_num` as the intended entry for\\n    that contact.\\n    \\\"\\\"\\\"\\n    return contact_df.drop_duplicates(subset=\\\"fc_num\\\", keep=\\\"last\\\"), people_df\\n\\n\\n@register_transform\\ndef lowercase_everything(contact_df, people_df):\\n    \\\"\\\"\\\"Convert all string columns to lowercase except `fc_num`, since capitalization is inconsistent throughout.\\\"\\\"\\\"\\n\\n    def to_lower(col):\\n        if col.name == \\\"fc_num\\\":\\n            return col\\n        return col.str.lower()\\n\\n    contact_df = contact_df.apply(to_lower)\\n    people_df = people_df.apply(to_lower)\\n\\n    return contact_df, people_df\\n\\n\\n@register_transform\\ndef rename_frisk_columns(contact_df, people_df):\\n    \\\"\\\"\\\"\\n    Combine the `frisked` and `searchperson` columns from the **contacts** table into one column called `fc_involved_frisk_or_search`,\\n    and disambiguate it from a related column in the **people** table by renaming the `frisk/search` column to `person_frisked_or_searched`.\\n    As noted on data.boston.gov, the `frisked` and `searchperson` columns (from the \\\"New RMS\\\" data system) indicate whether any of the \\n    individuals stopped in a given field contact was frisked, whereas `frisk/search` (from the \\\"Mark43\\\" data system) indicates whether\\n    a particular person involved in a field contact was frisked.\\n    \\\"\\\"\\\"\\n    combine_fields = (\\n        lambda row: \\\"y\\\"\\n        if row.frisked == \\\"y\\\" or row.searchperson == \\\"y\\\"\\n        else row.frisked\\n    )\\n    combined_values = contact_df[[\\\"frisked\\\", \\\"searchperson\\\"]].apply(\\n        combine_fields, axis=1\\n    )\\n    contact_df = contact_df.assign(fc_involved_frisk_or_search=combined_values)\\n    contact_df = contact_df.drop(columns=[\\\"frisked\\\", \\\"searchperson\\\"])\\n\\n    people_df = people_df.rename(columns={\\\"frisk/search\\\": \\\"person_frisked_or_searched\\\"})\\n    people_df.person_frisked_or_searched = people_df.person_frisked_or_searched.replace(\\n        {\\\"0\\\": \\\"n\\\", \\\"1\\\": \\\"y\\\"}\\n    )\\n\\n    # The newer system doesn't provide contact-level frisk info,\\n    # but we can infer whether a contact involved a frisk/search based on\\n    # whether the people involved in that contact were frisked.\\n    contacts_with_no_data = contact_df.fc_involved_frisk_or_search.isnull()\\n    contact_involved_frisk = people_df.groupby(\\\"fc_num\\\").apply(\\n        lambda g: \\\"y\\\"\\n        if (g.person_frisked_or_searched == \\\"y\\\").any()\\n        else \\\"n\\\"\\n        if (g.person_frisked_or_searched == \\\"n\\\").all()\\n        else np.nan\\n    )\\n    contact_df.loc[contacts_with_no_data, \\\"fc_involved_frisk_or_search\\\"] = contact_df[\\n        contacts_with_no_data\\n    ].apply(lambda row: contact_involved_frisk.loc[row.fc_num], axis=1)\\n\\n    return contact_df, people_df\\n\\n\\n@register_transform\\ndef vehicle_info_cleanup(contact_df, people_df):\\n    \\\"\\\"\\\"\\n    Clean up and reconcile vehicle-related field values from the **contacts** table. Across the different data systems, different text values\\n    were used to represent identical or overlapping concepts (e.g. \\\"LT. BLUE\\\" and \\\"light blue\\\", \\\"Suv (sport Utility Vehicle)\\\" \\n    and \\\"SUV or Utility Van\\\"). Also, drop the `vehicle_style` column, which is basically a noisier version of the `vehicle_type` column.\\n    \\\"\\\"\\\"\\n    contact_df.vehicle_type = contact_df.vehicle_type.replace(\\n        {\\n            \\\"scooter\\\": \\\"motorcycle or scooter\\\",\\n            \\\"cargo van\\\": \\\"suv or utility van\\\",\\n            \\\"suv (sport utility vehicle)\\\": \\\"suv or utility van\\\",\\n            \\\"passenger van\\\": \\\"bus or passenger van\\\",\\n            \\\"bus/passenger van\\\": \\\"bus or passenger van\\\",\\n            \\\"passenger car/ automobile\\\": \\\"passenger car\\\",\\n        }\\n    )\\n    contact_df.vehicle_color = contact_df.vehicle_color.str.strip().replace(\\n        {\\n            \\\"bla\\\": \\\"black\\\",\\n            \\\"gra\\\": \\\"gray\\\",\\n            \\\"gre\\\": \\\"green\\\",\\n            \\\"lt. green\\\": \\\"light green\\\",\\n            \\\"lt. blue\\\": \\\"light blue\\\",\\n        }\\n    )\\n\\n    def fix_year(year: str) -> str:\\n        if not year or year in (\\\"none\\\", \\\"null\\\"):\\n            return year\\n        num_year = float(year)\\n        if np.isnan(num_year):\\n            return year\\n        if num_year > 1900:\\n            return year\\n        if num_year > 99:\\n            # Exclude definite typos\\n            return \\\"null\\\"\\n        if num_year < 10:\\n            return f\\\"200{year}\\\"\\n        if num_year < 21:\\n            return f\\\"20{year}\\\"\\n        else:\\n            return f\\\"19{year}\\\"\\n\\n    contact_df.vehicle_year = contact_df.vehicle_year.apply(fix_year)\\n\\n    return contact_df, people_df\\n\\n\\n@register_transform\\ndef contact_date_to_dt(contact_df, people_df):\\n    \\\"\\\"\\\"\\n    Convert the `contact_date` column in the **contacts** table to datetime, \\n    and drop a duplicate column from the **people** table.\\n    \\\"\\\"\\\"\\n    contact_df.contact_date = pd.to_datetime(contact_df.contact_date)\\n    people_df = people_df.drop(columns=[\\\"contact_date\\\"])\\n\\n    return contact_df, people_df\\n\\n\\n@register_transform\\ndef clean_officer_and_supervisor_names(contact_df, people_df):\\n    \\\"\\\"\\\"\\n    Remove extra whitespace from the `contact_officer_name` column in the **contacts** table \\n    to reduce accidental duplication. Also, put officer's first names first.\\n    \\\"\\\"\\\"\\n    observed_names = {}\\n\\n    def clean_name(id: str, name: str) -> str:\\n        if id in observed_names:\\n            return observed_names[id]\\n\\n        name = name.replace(\\\".\\\", \\\"\\\")\\n        split_name = [part.strip() for part in name.split(\\\",\\\")]\\n        clean_name = None\\n        if len(split_name) == 1:\\n            clean_name = split_name[0]\\n        elif len(split_name) <= 3:\\n            last_part, *first_part = split_name\\n            clean_name = f'{\\\" \\\".join(first_part)} {last_part}'\\n\\n        if not clean_name:\\n            raise Exception(f\\\"Encountered name with unexpected structure: {name}\\\")\\n\\n        observed_names[id] = clean_name\\n        return clean_name\\n\\n    contact_df.supervisor_name = contact_df.apply(\\n        lambda row: clean_name(row.supervisor, row.supervisor_name), axis=1\\n    )\\n    contact_df.contact_officer_name = contact_df.apply(\\n        lambda row: clean_name(row.contact_officer, row.contact_officer_name), axis=1\\n    )\\n    return contact_df, people_df\\n\\n\\n@register_transform\\ndef combine_contact_reason_and_narrative(contact_df, people_df):\\n    \\\"\\\"\\\"\\n    Merge the **contacts** table's `contact_reason` column into the `narrative` column. `contact_reason` serves the same purpose as `narrative`,\\n    just for the older system. Uppercase both columns for consistency.\\n    \\\"\\\"\\\"\\n    empty_narrative = contact_df.narrative.isnull()\\n    contact_df.loc[empty_narrative, \\\"narrative\\\"] = contact_df[\\n        empty_narrative\\n    ].contact_reason\\n    contact_df = contact_df.drop(columns=[\\\"contact_reason\\\"])\\n    return contact_df, people_df\\n\\n\\n@register_transform\\ndef clean_up_city(contact_df, people_df):\\n    \\\"\\\"\\\"\\n    There are lots of typos and inconsistencies in the **contacts** table's `city` column, so fix them.\\n    Warning: current solution is not robust to *new* typos, should the data change.\\n    \\\"\\\"\\\"\\n    BOSTON = \\\"boston\\\"\\n    SOUTHIE = \\\"south boston\\\"\\n    DORCHESTER = \\\"dorchester\\\"\\n    CHARLESTOWN = \\\"charlestown\\\"\\n    JP = \\\"jamaica plain\\\"\\n    EASTIE = \\\"east boston\\\"\\n    MATTAPAN = \\\"mattapan\\\"\\n    ROXBURY = \\\"roxbury\\\"\\n    HYDEPARK = \\\"hyde park\\\"\\n    contact_df.city = contact_df.city.replace(\\n        {\\n            \\\"bstn\\\": BOSTON,\\n            \\\"so.boston\\\": SOUTHIE,\\n            \\\"dorcchester\\\": DORCHESTER,\\n            \\\"chaarlestown\\\": CHARLESTOWN,\\n            \\\"jp\\\": JP,\\n            \\\"east bos\\\": EASTIE,\\n            \\\"chalrestown\\\": CHARLESTOWN,\\n            \\\"east bostn\\\": EASTIE,\\n            \\\"dor\\\": DORCHESTER,\\n            \\\"mt\\\": MATTAPAN,\\n            \\\"s boston\\\": SOUTHIE,\\n            \\\"dorchster\\\": DORCHESTER,\\n            \\\"bst\\\": BOSTON,\\n            \\\"s bstn\\\": SOUTHIE,\\n            \\\"dorchesterr\\\": DORCHESTER,\\n            \\\"jamaiica plain\\\": JP,\\n            \\\"roxbury ma\\\": ROXBURY,\\n            \\\"so boston\\\": SOUTHIE,\\n            \\\"e. boston\\\": EASTIE,\\n            \\\"jamaica\\\": JP,\\n            \\\"ddorchester\\\": DORCHESTER,\\n            \\\"mattpan\\\": MATTAPAN,\\n            \\\"jamaicia\\\": JP,\\n            \\\"s. boston\\\": SOUTHIE,\\n            \\\"hp\\\": HYDEPARK,\\n            \\\"dorchest\\\": DORCHESTER,\\n            \\\"sbos\\\": SOUTHIE,\\n            \\\"rox\\\": ROXBURY,\\n            \\\"charlestwon\\\": CHARLESTOWN,\\n            \\\"jamacia plain\\\": JP,\\n            \\\"robury\\\": ROXBURY,\\n            \\\"btsn\\\": BOSTON,\\n            \\\"sommerville\\\": \\\"somerville\\\",\\n            \\\"jamaicia plain\\\": JP,\\n            \\\"s.boston\\\": SOUTHIE,\\n            \\\"dor.\\\": DORCHESTER,\\n            \\\"e boston\\\": EASTIE,\\n            \\\"e.boston\\\": EASTIE,\\n            \\\"bostob\\\": BOSTON,\\n            \\\"roslindlae\\\": \\\"ROSLINDALE\\\",\\n            \\\"bsnt\\\": BOSTON,\\n            \\\"bstna\\\": BOSTON,\\n            \\\"bston\\\": BOSTON,\\n            \\\"jamaiaca plain\\\": JP,\\n            \\\"so. boston\\\": SOUTHIE,\\n            \\\"unkown\\\": \\\"unknown\\\",\\n        }\\n    )\\n    return contact_df, people_df\\n\\n\\n@register_transform\\ndef clean_up_age(contact_df, people_df):\\n    \\\"\\\"\\\"Remove implausibly high values in the **people** table's `age` column. Also, convert string values to float.\\\"\\\"\\\"\\n    people_df.age = people_df.age.apply(\\n        lambda age: np.nan if len(age) > 2 or age == \\\"\\\" else np.float(age)\\n    )\\n    return contact_df, people_df\\n\\n\\n@register_transform\\ndef combine_skin_tone_and_complexion(contact_df, people_df):\\n    \\\"\\\"\\\"Merge the **people** table's older `complexion` column into the newer `skin_tone` column.\\\"\\\"\\\"\\n    empty_skin_tone = people_df.skin_tone.isnull()\\n    people_df.loc[empty_skin_tone, \\\"skin_tone\\\"] = people_df.complexion[empty_skin_tone]\\n    people_df.skin_tone = people_df.skin_tone\\n    people_df = people_df.drop(columns=[\\\"complexion\\\"])\\n    return contact_df, people_df\\n\\n\\n@register_transform\\ndef drop_deceased_column(contact_df, people_df):\\n    \\\"\\\"\\\"\\n    Drop the `deceased` column from the **people** table, since it doesn't exist in the older system, and since no one \\n    is marked deceased in this dataset.\\n    \\\"\\\"\\\"\\n    people_df = people_df.drop(columns=[\\\"deceased\\\"])\\n    return contact_df, people_df\\n\\n\\n@register_transform\\ndef reconcile_hair_style(contact_df, people_df):\\n    \\\"\\\"\\\"Reconcile the **people** table's `hair_style` values that seem to mean the same thing.\\\"\\\"\\\"\\n    people_df.hair_style = people_df.hair_style.replace(\\n        {\\n            \\\"receding / thin\\\": \\\"receding or thin\\\",\\n            \\\"receding or slightly receding\\\": \\\"receding or thin\\\",\\n            \\\"bald\\\": \\\"bald or balding\\\",\\n            \\\"braids\\\": \\\"braided\\\",\\n            \\\"wig/hair piece\\\": \\\"wig or hair piece\\\",\\n        }\\n    )\\n    return contact_df, people_df\\n\\n\\n@register_transform\\ndef reconcile_race(contact_df, people_df):\\n    \\\"\\\"\\\"Reconcile **people** table `race` values that seem to mean the same thing.\\\"\\\"\\\"\\n    people_df.race = people_df.race.replace(\\n        {\\\"american indian or alaskan native\\\": \\\"native american / alaskan native\\\"}\\n    )\\n    return contact_df, people_df\\n\\n\\n@register_transform\\ndef clean_up_state(contact_df, people_df):\\n    \\\"\\\"\\\"Fix recurrent typo in the **contacts** table `state` column.\\\"\\\"\\\"\\n    contact_df.state = contact_df.state.replace({\\\"MX\\\": \\\"MA\\\"})\\n    return contact_df, people_df\\n\\n\\n@register_transform\\ndef reconcile_stop_duration(contact_df, people_df):\\n    \\\"\\\"\\\"Bucket **contacts** table `stop_duration` column values listed as minutes and remove likely typos.\\\"\\\"\\\"\\n\\n    def bucket_stop_duration(d: str) -> str:\\n        try:\\n            d = int(d)\\n        except:\\n            return d\\n\\n        # A stop longer than 8 hours seems really implausible\\n        if d > 500:\\n            return \\\"null\\\"\\n        if d < 5:\\n            return \\\"less than five minutes\\\"\\n        if d < 10:\\n            return \\\"five to ten minutes\\\"\\n        if d < 15:\\n            return \\\"ten to fifteen minutes\\\"\\n        if d < 20:\\n            return \\\"fifteen to twenty minutes\\\"\\n        if d < 25:\\n            return \\\"twenty to twenty-five minutes\\\"\\n        if d < 30:\\n            return \\\"twenty-five to thirty minutes\\\"\\n        if d < 45:\\n            return \\\"thirty to forty-five minutes\\\"\\n        if d < 60:\\n            return \\\"forty-five to sixty minutes\\\"\\n        if d < 120:\\n            return \\\"one to two hours\\\"\\n        return \\\"longer than two hours\\\"\\n\\n    contact_df.stop_duration = contact_df.stop_duration.apply(bucket_stop_duration)\\n    return contact_df, people_df\\n\\n\\n@register_transform\\ndef clean_location_info(contact_df, people_df):\\n    \\\"\\\"\\\"\\n    Combine the **contacts** table's `street` and `streetaddr` columns into single `street` column, \\n    drop `streedaddr`, and trim extra digits from `zip`.\\n    \\\"\\\"\\\"\\n    empty_street = contact_df.street.isnull()\\n    contact_df.loc[empty_street, \\\"street\\\"] = contact_df.streetaddr[empty_street]\\n    contact_df.street = contact_df.street.apply(lambda s: s.replace(\\\"&\\\", \\\"/\\\"))\\n\\n    contact_df = contact_df.drop(columns=[\\\"streetaddr\\\"])\\n\\n    contact_df.zip = contact_df.zip.apply(lambda zip: zip.split(\\\"-\\\")[0])\\n\\n    return contact_df, people_df\\n\\n\\n@register_transform\\ndef clean_empty_values(contact_df, people_df) -> pd.DataFrame:\\n    \\\"\\\"\\\"\\n    The values `''`, `'NULL'`, and `'None'` are used to signify no value was entered in various fields across the dataset.\\n    Replace these values with `np.nan`.\\n    \\\"\\\"\\\"\\n    replacements = {\\\"\\\": np.nan, \\\"null\\\": np.nan, \\\"none\\\": np.nan}\\n    contact_df = contact_df.replace(replacements)\\n    people_df = people_df.replace(replacements)\\n    return contact_df, people_df\\n\\n\\nclean_contact_df, clean_people_df = apply_transforms(contact_df, people_df)\";\n",
       "                var nbb_formatted_code = \"df_transforms = []\\n\\n\\ndef register_transform(f: callable) -> callable:\\n    df_transforms.append(f)\\n    return f\\n\\n\\ndef apply_transforms(\\n    contact_df: pd.DataFrame, people_df: pd.DataFrame\\n) -> (pd.DataFrame, pd.DataFrame):\\n    for t in df_transforms:\\n        contact_df, people_df = t(contact_df, people_df)\\n    return contact_df, people_df\\n\\n\\n@register_transform\\ndef deduplicate_fc_num(contact_df, people_df):\\n    \\\"\\\"\\\"\\n    Around 25 records in the **contacts** table contain duplicate `fc_num`s, which is supposed to be a unique \\n    identifier for a given stop. Manual inspection of the data suggests this occurs when officers update the `basis`\\n    field for a contact report after initially entering it, but this is just my off-the-cuff guess. Based on \\n    this guess, remove duplicates by taking that last record with a duplicated `fc_num` as the intended entry for\\n    that contact.\\n    \\\"\\\"\\\"\\n    return contact_df.drop_duplicates(subset=\\\"fc_num\\\", keep=\\\"last\\\"), people_df\\n\\n\\n@register_transform\\ndef lowercase_everything(contact_df, people_df):\\n    \\\"\\\"\\\"Convert all string columns to lowercase except `fc_num`, since capitalization is inconsistent throughout.\\\"\\\"\\\"\\n\\n    def to_lower(col):\\n        if col.name == \\\"fc_num\\\":\\n            return col\\n        return col.str.lower()\\n\\n    contact_df = contact_df.apply(to_lower)\\n    people_df = people_df.apply(to_lower)\\n\\n    return contact_df, people_df\\n\\n\\n@register_transform\\ndef rename_frisk_columns(contact_df, people_df):\\n    \\\"\\\"\\\"\\n    Combine the `frisked` and `searchperson` columns from the **contacts** table into one column called `fc_involved_frisk_or_search`,\\n    and disambiguate it from a related column in the **people** table by renaming the `frisk/search` column to `person_frisked_or_searched`.\\n    As noted on data.boston.gov, the `frisked` and `searchperson` columns (from the \\\"New RMS\\\" data system) indicate whether any of the \\n    individuals stopped in a given field contact was frisked, whereas `frisk/search` (from the \\\"Mark43\\\" data system) indicates whether\\n    a particular person involved in a field contact was frisked.\\n    \\\"\\\"\\\"\\n    combine_fields = (\\n        lambda row: \\\"y\\\"\\n        if row.frisked == \\\"y\\\" or row.searchperson == \\\"y\\\"\\n        else row.frisked\\n    )\\n    combined_values = contact_df[[\\\"frisked\\\", \\\"searchperson\\\"]].apply(\\n        combine_fields, axis=1\\n    )\\n    contact_df = contact_df.assign(fc_involved_frisk_or_search=combined_values)\\n    contact_df = contact_df.drop(columns=[\\\"frisked\\\", \\\"searchperson\\\"])\\n\\n    people_df = people_df.rename(columns={\\\"frisk/search\\\": \\\"person_frisked_or_searched\\\"})\\n    people_df.person_frisked_or_searched = people_df.person_frisked_or_searched.replace(\\n        {\\\"0\\\": \\\"n\\\", \\\"1\\\": \\\"y\\\"}\\n    )\\n\\n    # The newer system doesn't provide contact-level frisk info,\\n    # but we can infer whether a contact involved a frisk/search based on\\n    # whether the people involved in that contact were frisked.\\n    contacts_with_no_data = contact_df.fc_involved_frisk_or_search.isnull()\\n    contact_involved_frisk = people_df.groupby(\\\"fc_num\\\").apply(\\n        lambda g: \\\"y\\\"\\n        if (g.person_frisked_or_searched == \\\"y\\\").any()\\n        else \\\"n\\\"\\n        if (g.person_frisked_or_searched == \\\"n\\\").all()\\n        else np.nan\\n    )\\n    contact_df.loc[contacts_with_no_data, \\\"fc_involved_frisk_or_search\\\"] = contact_df[\\n        contacts_with_no_data\\n    ].apply(lambda row: contact_involved_frisk.loc[row.fc_num], axis=1)\\n\\n    return contact_df, people_df\\n\\n\\n@register_transform\\ndef vehicle_info_cleanup(contact_df, people_df):\\n    \\\"\\\"\\\"\\n    Clean up and reconcile vehicle-related field values from the **contacts** table. Across the different data systems, different text values\\n    were used to represent identical or overlapping concepts (e.g. \\\"LT. BLUE\\\" and \\\"light blue\\\", \\\"Suv (sport Utility Vehicle)\\\" \\n    and \\\"SUV or Utility Van\\\"). Also, drop the `vehicle_style` column, which is basically a noisier version of the `vehicle_type` column.\\n    \\\"\\\"\\\"\\n    contact_df.vehicle_type = contact_df.vehicle_type.replace(\\n        {\\n            \\\"scooter\\\": \\\"motorcycle or scooter\\\",\\n            \\\"cargo van\\\": \\\"suv or utility van\\\",\\n            \\\"suv (sport utility vehicle)\\\": \\\"suv or utility van\\\",\\n            \\\"passenger van\\\": \\\"bus or passenger van\\\",\\n            \\\"bus/passenger van\\\": \\\"bus or passenger van\\\",\\n            \\\"passenger car/ automobile\\\": \\\"passenger car\\\",\\n        }\\n    )\\n    contact_df.vehicle_color = contact_df.vehicle_color.str.strip().replace(\\n        {\\n            \\\"bla\\\": \\\"black\\\",\\n            \\\"gra\\\": \\\"gray\\\",\\n            \\\"gre\\\": \\\"green\\\",\\n            \\\"lt. green\\\": \\\"light green\\\",\\n            \\\"lt. blue\\\": \\\"light blue\\\",\\n        }\\n    )\\n\\n    def fix_year(year: str) -> str:\\n        if not year or year in (\\\"none\\\", \\\"null\\\"):\\n            return year\\n        num_year = float(year)\\n        if np.isnan(num_year):\\n            return year\\n        if num_year > 1900:\\n            return year\\n        if num_year > 99:\\n            # Exclude definite typos\\n            return \\\"null\\\"\\n        if num_year < 10:\\n            return f\\\"200{year}\\\"\\n        if num_year < 21:\\n            return f\\\"20{year}\\\"\\n        else:\\n            return f\\\"19{year}\\\"\\n\\n    contact_df.vehicle_year = contact_df.vehicle_year.apply(fix_year)\\n\\n    return contact_df, people_df\\n\\n\\n@register_transform\\ndef contact_date_to_dt(contact_df, people_df):\\n    \\\"\\\"\\\"\\n    Convert the `contact_date` column in the **contacts** table to datetime, \\n    and drop a duplicate column from the **people** table.\\n    \\\"\\\"\\\"\\n    contact_df.contact_date = pd.to_datetime(contact_df.contact_date)\\n    people_df = people_df.drop(columns=[\\\"contact_date\\\"])\\n\\n    return contact_df, people_df\\n\\n\\n@register_transform\\ndef clean_officer_and_supervisor_names(contact_df, people_df):\\n    \\\"\\\"\\\"\\n    Remove extra whitespace from the `contact_officer_name` column in the **contacts** table \\n    to reduce accidental duplication. Also, put officer's first names first.\\n    \\\"\\\"\\\"\\n    observed_names = {}\\n\\n    def clean_name(id: str, name: str) -> str:\\n        if id in observed_names:\\n            return observed_names[id]\\n\\n        name = name.replace(\\\".\\\", \\\"\\\")\\n        split_name = [part.strip() for part in name.split(\\\",\\\")]\\n        clean_name = None\\n        if len(split_name) == 1:\\n            clean_name = split_name[0]\\n        elif len(split_name) <= 3:\\n            last_part, *first_part = split_name\\n            clean_name = f'{\\\" \\\".join(first_part)} {last_part}'\\n\\n        if not clean_name:\\n            raise Exception(f\\\"Encountered name with unexpected structure: {name}\\\")\\n\\n        observed_names[id] = clean_name\\n        return clean_name\\n\\n    contact_df.supervisor_name = contact_df.apply(\\n        lambda row: clean_name(row.supervisor, row.supervisor_name), axis=1\\n    )\\n    contact_df.contact_officer_name = contact_df.apply(\\n        lambda row: clean_name(row.contact_officer, row.contact_officer_name), axis=1\\n    )\\n    return contact_df, people_df\\n\\n\\n@register_transform\\ndef combine_contact_reason_and_narrative(contact_df, people_df):\\n    \\\"\\\"\\\"\\n    Merge the **contacts** table's `contact_reason` column into the `narrative` column. `contact_reason` serves the same purpose as `narrative`,\\n    just for the older system. Uppercase both columns for consistency.\\n    \\\"\\\"\\\"\\n    empty_narrative = contact_df.narrative.isnull()\\n    contact_df.loc[empty_narrative, \\\"narrative\\\"] = contact_df[\\n        empty_narrative\\n    ].contact_reason\\n    contact_df = contact_df.drop(columns=[\\\"contact_reason\\\"])\\n    return contact_df, people_df\\n\\n\\n@register_transform\\ndef clean_up_city(contact_df, people_df):\\n    \\\"\\\"\\\"\\n    There are lots of typos and inconsistencies in the **contacts** table's `city` column, so fix them.\\n    Warning: current solution is not robust to *new* typos, should the data change.\\n    \\\"\\\"\\\"\\n    BOSTON = \\\"boston\\\"\\n    SOUTHIE = \\\"south boston\\\"\\n    DORCHESTER = \\\"dorchester\\\"\\n    CHARLESTOWN = \\\"charlestown\\\"\\n    JP = \\\"jamaica plain\\\"\\n    EASTIE = \\\"east boston\\\"\\n    MATTAPAN = \\\"mattapan\\\"\\n    ROXBURY = \\\"roxbury\\\"\\n    HYDEPARK = \\\"hyde park\\\"\\n    contact_df.city = contact_df.city.replace(\\n        {\\n            \\\"bstn\\\": BOSTON,\\n            \\\"so.boston\\\": SOUTHIE,\\n            \\\"dorcchester\\\": DORCHESTER,\\n            \\\"chaarlestown\\\": CHARLESTOWN,\\n            \\\"jp\\\": JP,\\n            \\\"east bos\\\": EASTIE,\\n            \\\"chalrestown\\\": CHARLESTOWN,\\n            \\\"east bostn\\\": EASTIE,\\n            \\\"dor\\\": DORCHESTER,\\n            \\\"mt\\\": MATTAPAN,\\n            \\\"s boston\\\": SOUTHIE,\\n            \\\"dorchster\\\": DORCHESTER,\\n            \\\"bst\\\": BOSTON,\\n            \\\"s bstn\\\": SOUTHIE,\\n            \\\"dorchesterr\\\": DORCHESTER,\\n            \\\"jamaiica plain\\\": JP,\\n            \\\"roxbury ma\\\": ROXBURY,\\n            \\\"so boston\\\": SOUTHIE,\\n            \\\"e. boston\\\": EASTIE,\\n            \\\"jamaica\\\": JP,\\n            \\\"ddorchester\\\": DORCHESTER,\\n            \\\"mattpan\\\": MATTAPAN,\\n            \\\"jamaicia\\\": JP,\\n            \\\"s. boston\\\": SOUTHIE,\\n            \\\"hp\\\": HYDEPARK,\\n            \\\"dorchest\\\": DORCHESTER,\\n            \\\"sbos\\\": SOUTHIE,\\n            \\\"rox\\\": ROXBURY,\\n            \\\"charlestwon\\\": CHARLESTOWN,\\n            \\\"jamacia plain\\\": JP,\\n            \\\"robury\\\": ROXBURY,\\n            \\\"btsn\\\": BOSTON,\\n            \\\"sommerville\\\": \\\"somerville\\\",\\n            \\\"jamaicia plain\\\": JP,\\n            \\\"s.boston\\\": SOUTHIE,\\n            \\\"dor.\\\": DORCHESTER,\\n            \\\"e boston\\\": EASTIE,\\n            \\\"e.boston\\\": EASTIE,\\n            \\\"bostob\\\": BOSTON,\\n            \\\"roslindlae\\\": \\\"ROSLINDALE\\\",\\n            \\\"bsnt\\\": BOSTON,\\n            \\\"bstna\\\": BOSTON,\\n            \\\"bston\\\": BOSTON,\\n            \\\"jamaiaca plain\\\": JP,\\n            \\\"so. boston\\\": SOUTHIE,\\n            \\\"unkown\\\": \\\"unknown\\\",\\n        }\\n    )\\n    return contact_df, people_df\\n\\n\\n@register_transform\\ndef clean_up_age(contact_df, people_df):\\n    \\\"\\\"\\\"Remove implausibly high values in the **people** table's `age` column. Also, convert string values to float.\\\"\\\"\\\"\\n    people_df.age = people_df.age.apply(\\n        lambda age: np.nan if len(age) > 2 or age == \\\"\\\" else np.float(age)\\n    )\\n    return contact_df, people_df\\n\\n\\n@register_transform\\ndef combine_skin_tone_and_complexion(contact_df, people_df):\\n    \\\"\\\"\\\"Merge the **people** table's older `complexion` column into the newer `skin_tone` column.\\\"\\\"\\\"\\n    empty_skin_tone = people_df.skin_tone.isnull()\\n    people_df.loc[empty_skin_tone, \\\"skin_tone\\\"] = people_df.complexion[empty_skin_tone]\\n    people_df.skin_tone = people_df.skin_tone\\n    people_df = people_df.drop(columns=[\\\"complexion\\\"])\\n    return contact_df, people_df\\n\\n\\n@register_transform\\ndef drop_deceased_column(contact_df, people_df):\\n    \\\"\\\"\\\"\\n    Drop the `deceased` column from the **people** table, since it doesn't exist in the older system, and since no one \\n    is marked deceased in this dataset.\\n    \\\"\\\"\\\"\\n    people_df = people_df.drop(columns=[\\\"deceased\\\"])\\n    return contact_df, people_df\\n\\n\\n@register_transform\\ndef reconcile_hair_style(contact_df, people_df):\\n    \\\"\\\"\\\"Reconcile the **people** table's `hair_style` values that seem to mean the same thing.\\\"\\\"\\\"\\n    people_df.hair_style = people_df.hair_style.replace(\\n        {\\n            \\\"receding / thin\\\": \\\"receding or thin\\\",\\n            \\\"receding or slightly receding\\\": \\\"receding or thin\\\",\\n            \\\"bald\\\": \\\"bald or balding\\\",\\n            \\\"braids\\\": \\\"braided\\\",\\n            \\\"wig/hair piece\\\": \\\"wig or hair piece\\\",\\n        }\\n    )\\n    return contact_df, people_df\\n\\n\\n@register_transform\\ndef reconcile_race(contact_df, people_df):\\n    \\\"\\\"\\\"Reconcile **people** table `race` values that seem to mean the same thing.\\\"\\\"\\\"\\n    people_df.race = people_df.race.replace(\\n        {\\\"american indian or alaskan native\\\": \\\"native american / alaskan native\\\"}\\n    )\\n    return contact_df, people_df\\n\\n\\n@register_transform\\ndef clean_up_state(contact_df, people_df):\\n    \\\"\\\"\\\"Fix recurrent typo in the **contacts** table `state` column.\\\"\\\"\\\"\\n    contact_df.state = contact_df.state.replace({\\\"MX\\\": \\\"MA\\\"})\\n    return contact_df, people_df\\n\\n\\n@register_transform\\ndef reconcile_stop_duration(contact_df, people_df):\\n    \\\"\\\"\\\"Bucket **contacts** table `stop_duration` column values listed as minutes and remove likely typos.\\\"\\\"\\\"\\n\\n    def bucket_stop_duration(d: str) -> str:\\n        try:\\n            d = int(d)\\n        except:\\n            return d\\n\\n        # A stop longer than 8 hours seems really implausible\\n        if d > 500:\\n            return \\\"null\\\"\\n        if d < 5:\\n            return \\\"less than five minutes\\\"\\n        if d < 10:\\n            return \\\"five to ten minutes\\\"\\n        if d < 15:\\n            return \\\"ten to fifteen minutes\\\"\\n        if d < 20:\\n            return \\\"fifteen to twenty minutes\\\"\\n        if d < 25:\\n            return \\\"twenty to twenty-five minutes\\\"\\n        if d < 30:\\n            return \\\"twenty-five to thirty minutes\\\"\\n        if d < 45:\\n            return \\\"thirty to forty-five minutes\\\"\\n        if d < 60:\\n            return \\\"forty-five to sixty minutes\\\"\\n        if d < 120:\\n            return \\\"one to two hours\\\"\\n        return \\\"longer than two hours\\\"\\n\\n    contact_df.stop_duration = contact_df.stop_duration.apply(bucket_stop_duration)\\n    return contact_df, people_df\\n\\n\\n@register_transform\\ndef clean_location_info(contact_df, people_df):\\n    \\\"\\\"\\\"\\n    Combine the **contacts** table's `street` and `streetaddr` columns into single `street` column, \\n    drop `streedaddr`, and trim extra digits from `zip`.\\n    \\\"\\\"\\\"\\n    empty_street = contact_df.street.isnull()\\n    contact_df.loc[empty_street, \\\"street\\\"] = contact_df.streetaddr[empty_street]\\n    contact_df.street = contact_df.street.apply(lambda s: s.replace(\\\"&\\\", \\\"/\\\"))\\n\\n    contact_df = contact_df.drop(columns=[\\\"streetaddr\\\"])\\n\\n    contact_df.zip = contact_df.zip.apply(lambda zip: zip.split(\\\"-\\\")[0])\\n\\n    return contact_df, people_df\\n\\n\\n@register_transform\\ndef clean_empty_values(contact_df, people_df) -> pd.DataFrame:\\n    \\\"\\\"\\\"\\n    The values `''`, `'NULL'`, and `'None'` are used to signify no value was entered in various fields across the dataset.\\n    Replace these values with `np.nan`.\\n    \\\"\\\"\\\"\\n    replacements = {\\\"\\\": np.nan, \\\"null\\\": np.nan, \\\"none\\\": np.nan}\\n    contact_df = contact_df.replace(replacements)\\n    people_df = people_df.replace(replacements)\\n    return contact_df, people_df\\n\\n\\nclean_contact_df, clean_people_df = apply_transforms(contact_df, people_df)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_transforms = []\n",
    "\n",
    "\n",
    "def register_transform(f: callable) -> callable:\n",
    "    df_transforms.append(f)\n",
    "    return f\n",
    "\n",
    "\n",
    "def apply_transforms(\n",
    "    contact_df: pd.DataFrame, people_df: pd.DataFrame\n",
    ") -> (pd.DataFrame, pd.DataFrame):\n",
    "    for t in df_transforms:\n",
    "        contact_df, people_df = t(contact_df, people_df)\n",
    "    return contact_df, people_df\n",
    "\n",
    "\n",
    "@register_transform\n",
    "def deduplicate_fc_num(contact_df, people_df):\n",
    "    \"\"\"\n",
    "    Around 25 records in the **contacts** table contain duplicate `fc_num`s, which is supposed to be a unique \n",
    "    identifier for a given stop. Manual inspection of the data suggests this occurs when officers update the `basis`\n",
    "    field for a contact report after initially entering it, but this is just my off-the-cuff guess. Based on \n",
    "    this guess, remove duplicates by taking that last record with a duplicated `fc_num` as the intended entry for\n",
    "    that contact.\n",
    "    \"\"\"\n",
    "    return contact_df.drop_duplicates(subset=\"fc_num\", keep=\"last\"), people_df\n",
    "\n",
    "\n",
    "@register_transform\n",
    "def lowercase_everything(contact_df, people_df):\n",
    "    \"\"\"Convert all string columns to lowercase except `fc_num`, since capitalization is inconsistent throughout.\"\"\"\n",
    "\n",
    "    def to_lower(col):\n",
    "        if col.name == \"fc_num\":\n",
    "            return col\n",
    "        return col.str.lower()\n",
    "\n",
    "    contact_df = contact_df.apply(to_lower)\n",
    "    people_df = people_df.apply(to_lower)\n",
    "\n",
    "    return contact_df, people_df\n",
    "\n",
    "\n",
    "@register_transform\n",
    "def rename_frisk_columns(contact_df, people_df):\n",
    "    \"\"\"\n",
    "    Combine the `frisked` and `searchperson` columns from the **contacts** table into one column called `fc_involved_frisk_or_search`,\n",
    "    and disambiguate it from a related column in the **people** table by renaming the `frisk/search` column to `person_frisked_or_searched`.\n",
    "    As noted on data.boston.gov, the `frisked` and `searchperson` columns (from the \"New RMS\" data system) indicate whether any of the \n",
    "    individuals stopped in a given field contact was frisked, whereas `frisk/search` (from the \"Mark43\" data system) indicates whether\n",
    "    a particular person involved in a field contact was frisked.\n",
    "    \"\"\"\n",
    "    combine_fields = (\n",
    "        lambda row: \"y\"\n",
    "        if row.frisked == \"y\" or row.searchperson == \"y\"\n",
    "        else row.frisked\n",
    "    )\n",
    "    combined_values = contact_df[[\"frisked\", \"searchperson\"]].apply(\n",
    "        combine_fields, axis=1\n",
    "    )\n",
    "    contact_df = contact_df.assign(fc_involved_frisk_or_search=combined_values)\n",
    "    contact_df = contact_df.drop(columns=[\"frisked\", \"searchperson\"])\n",
    "\n",
    "    people_df = people_df.rename(columns={\"frisk/search\": \"person_frisked_or_searched\"})\n",
    "    people_df.person_frisked_or_searched = people_df.person_frisked_or_searched.replace(\n",
    "        {\"0\": \"n\", \"1\": \"y\"}\n",
    "    )\n",
    "\n",
    "    # The newer system doesn't provide contact-level frisk info,\n",
    "    # but we can infer whether a contact involved a frisk/search based on\n",
    "    # whether the people involved in that contact were frisked.\n",
    "    contacts_with_no_data = contact_df.fc_involved_frisk_or_search.isnull()\n",
    "    contact_involved_frisk = people_df.groupby(\"fc_num\").apply(\n",
    "        lambda g: \"y\"\n",
    "        if (g.person_frisked_or_searched == \"y\").any()\n",
    "        else \"n\"\n",
    "        if (g.person_frisked_or_searched == \"n\").all()\n",
    "        else np.nan\n",
    "    )\n",
    "    contact_df.loc[contacts_with_no_data, \"fc_involved_frisk_or_search\"] = contact_df[\n",
    "        contacts_with_no_data\n",
    "    ].apply(lambda row: contact_involved_frisk.loc[row.fc_num], axis=1)\n",
    "\n",
    "    return contact_df, people_df\n",
    "\n",
    "\n",
    "@register_transform\n",
    "def vehicle_info_cleanup(contact_df, people_df):\n",
    "    \"\"\"\n",
    "    Clean up and reconcile vehicle-related field values from the **contacts** table. Across the different data systems, different text values\n",
    "    were used to represent identical or overlapping concepts (e.g. \"LT. BLUE\" and \"light blue\", \"Suv (sport Utility Vehicle)\" \n",
    "    and \"SUV or Utility Van\"). Also, drop the `vehicle_style` column, which is basically a noisier version of the `vehicle_type` column.\n",
    "    \"\"\"\n",
    "    contact_df.vehicle_type = contact_df.vehicle_type.replace(\n",
    "        {\n",
    "            \"scooter\": \"motorcycle or scooter\",\n",
    "            \"cargo van\": \"suv or utility van\",\n",
    "            \"suv (sport utility vehicle)\": \"suv or utility van\",\n",
    "            \"passenger van\": \"bus or passenger van\",\n",
    "            \"bus/passenger van\": \"bus or passenger van\",\n",
    "            \"passenger car/ automobile\": \"passenger car\",\n",
    "        }\n",
    "    )\n",
    "    contact_df.vehicle_color = contact_df.vehicle_color.str.strip().replace(\n",
    "        {\n",
    "            \"bla\": \"black\",\n",
    "            \"gra\": \"gray\",\n",
    "            \"gre\": \"green\",\n",
    "            \"lt. green\": \"light green\",\n",
    "            \"lt. blue\": \"light blue\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    def fix_year(year: str) -> str:\n",
    "        if not year or year in (\"none\", \"null\"):\n",
    "            return year\n",
    "        num_year = float(year)\n",
    "        if np.isnan(num_year):\n",
    "            return year\n",
    "        if num_year > 1900:\n",
    "            return year\n",
    "        if num_year > 99:\n",
    "            # Exclude definite typos\n",
    "            return \"null\"\n",
    "        if num_year < 10:\n",
    "            return f\"200{year}\"\n",
    "        if num_year < 21:\n",
    "            return f\"20{year}\"\n",
    "        else:\n",
    "            return f\"19{year}\"\n",
    "\n",
    "    contact_df.vehicle_year = contact_df.vehicle_year.apply(fix_year)\n",
    "\n",
    "    return contact_df, people_df\n",
    "\n",
    "\n",
    "@register_transform\n",
    "def contact_date_to_dt(contact_df, people_df):\n",
    "    \"\"\"\n",
    "    Convert the `contact_date` column in the **contacts** table to datetime, \n",
    "    and drop a duplicate column from the **people** table.\n",
    "    \"\"\"\n",
    "    contact_df.contact_date = pd.to_datetime(contact_df.contact_date)\n",
    "    people_df = people_df.drop(columns=[\"contact_date\"])\n",
    "\n",
    "    return contact_df, people_df\n",
    "\n",
    "\n",
    "@register_transform\n",
    "def clean_officer_and_supervisor_names(contact_df, people_df):\n",
    "    \"\"\"\n",
    "    Remove extra whitespace from the `contact_officer_name` column in the **contacts** table \n",
    "    to reduce accidental duplication. Also, put officer's first names first.\n",
    "    \"\"\"\n",
    "    observed_names = {}\n",
    "\n",
    "    def clean_name(id: str, name: str) -> str:\n",
    "        if id in observed_names:\n",
    "            return observed_names[id]\n",
    "\n",
    "        name = name.replace(\".\", \"\")\n",
    "        split_name = [part.strip() for part in name.split(\",\")]\n",
    "        clean_name = None\n",
    "        if len(split_name) == 1:\n",
    "            clean_name = split_name[0]\n",
    "        elif len(split_name) <= 3:\n",
    "            last_part, *first_part = split_name\n",
    "            clean_name = f'{\" \".join(first_part)} {last_part}'\n",
    "\n",
    "        if not clean_name:\n",
    "            raise Exception(f\"Encountered name with unexpected structure: {name}\")\n",
    "\n",
    "        observed_names[id] = clean_name\n",
    "        return clean_name\n",
    "\n",
    "    contact_df.supervisor_name = contact_df.apply(\n",
    "        lambda row: clean_name(row.supervisor, row.supervisor_name), axis=1\n",
    "    )\n",
    "    contact_df.contact_officer_name = contact_df.apply(\n",
    "        lambda row: clean_name(row.contact_officer, row.contact_officer_name), axis=1\n",
    "    )\n",
    "    return contact_df, people_df\n",
    "\n",
    "\n",
    "@register_transform\n",
    "def combine_contact_reason_and_narrative(contact_df, people_df):\n",
    "    \"\"\"\n",
    "    Merge the **contacts** table's `contact_reason` column into the `narrative` column. `contact_reason` serves the same purpose as `narrative`,\n",
    "    just for the older system. Uppercase both columns for consistency.\n",
    "    \"\"\"\n",
    "    empty_narrative = contact_df.narrative.isnull()\n",
    "    contact_df.loc[empty_narrative, \"narrative\"] = contact_df[\n",
    "        empty_narrative\n",
    "    ].contact_reason\n",
    "    contact_df = contact_df.drop(columns=[\"contact_reason\"])\n",
    "    return contact_df, people_df\n",
    "\n",
    "\n",
    "@register_transform\n",
    "def clean_up_city(contact_df, people_df):\n",
    "    \"\"\"\n",
    "    There are lots of typos and inconsistencies in the **contacts** table's `city` column, so fix them.\n",
    "    Warning: current solution is not robust to *new* typos, should the data change.\n",
    "    \"\"\"\n",
    "    BOSTON = \"boston\"\n",
    "    SOUTHIE = \"south boston\"\n",
    "    DORCHESTER = \"dorchester\"\n",
    "    CHARLESTOWN = \"charlestown\"\n",
    "    JP = \"jamaica plain\"\n",
    "    EASTIE = \"east boston\"\n",
    "    MATTAPAN = \"mattapan\"\n",
    "    ROXBURY = \"roxbury\"\n",
    "    HYDEPARK = \"hyde park\"\n",
    "    contact_df.city = contact_df.city.replace(\n",
    "        {\n",
    "            \"bstn\": BOSTON,\n",
    "            \"so.boston\": SOUTHIE,\n",
    "            \"dorcchester\": DORCHESTER,\n",
    "            \"chaarlestown\": CHARLESTOWN,\n",
    "            \"jp\": JP,\n",
    "            \"east bos\": EASTIE,\n",
    "            \"chalrestown\": CHARLESTOWN,\n",
    "            \"east bostn\": EASTIE,\n",
    "            \"dor\": DORCHESTER,\n",
    "            \"mt\": MATTAPAN,\n",
    "            \"s boston\": SOUTHIE,\n",
    "            \"dorchster\": DORCHESTER,\n",
    "            \"bst\": BOSTON,\n",
    "            \"s bstn\": SOUTHIE,\n",
    "            \"dorchesterr\": DORCHESTER,\n",
    "            \"jamaiica plain\": JP,\n",
    "            \"roxbury ma\": ROXBURY,\n",
    "            \"so boston\": SOUTHIE,\n",
    "            \"e. boston\": EASTIE,\n",
    "            \"jamaica\": JP,\n",
    "            \"ddorchester\": DORCHESTER,\n",
    "            \"mattpan\": MATTAPAN,\n",
    "            \"jamaicia\": JP,\n",
    "            \"s. boston\": SOUTHIE,\n",
    "            \"hp\": HYDEPARK,\n",
    "            \"dorchest\": DORCHESTER,\n",
    "            \"sbos\": SOUTHIE,\n",
    "            \"rox\": ROXBURY,\n",
    "            \"charlestwon\": CHARLESTOWN,\n",
    "            \"jamacia plain\": JP,\n",
    "            \"robury\": ROXBURY,\n",
    "            \"btsn\": BOSTON,\n",
    "            \"sommerville\": \"somerville\",\n",
    "            \"jamaicia plain\": JP,\n",
    "            \"s.boston\": SOUTHIE,\n",
    "            \"dor.\": DORCHESTER,\n",
    "            \"e boston\": EASTIE,\n",
    "            \"e.boston\": EASTIE,\n",
    "            \"bostob\": BOSTON,\n",
    "            \"roslindlae\": \"ROSLINDALE\",\n",
    "            \"bsnt\": BOSTON,\n",
    "            \"bstna\": BOSTON,\n",
    "            \"bston\": BOSTON,\n",
    "            \"jamaiaca plain\": JP,\n",
    "            \"so. boston\": SOUTHIE,\n",
    "            \"unkown\": \"unknown\",\n",
    "        }\n",
    "    )\n",
    "    return contact_df, people_df\n",
    "\n",
    "\n",
    "@register_transform\n",
    "def clean_up_age(contact_df, people_df):\n",
    "    \"\"\"Remove implausibly high values in the **people** table's `age` column. Also, convert string values to float.\"\"\"\n",
    "    people_df.age = people_df.age.apply(\n",
    "        lambda age: np.nan if len(age) > 2 or age == \"\" else np.float(age)\n",
    "    )\n",
    "    return contact_df, people_df\n",
    "\n",
    "\n",
    "@register_transform\n",
    "def combine_skin_tone_and_complexion(contact_df, people_df):\n",
    "    \"\"\"Merge the **people** table's older `complexion` column into the newer `skin_tone` column.\"\"\"\n",
    "    empty_skin_tone = people_df.skin_tone.isnull()\n",
    "    people_df.loc[empty_skin_tone, \"skin_tone\"] = people_df.complexion[empty_skin_tone]\n",
    "    people_df.skin_tone = people_df.skin_tone\n",
    "    people_df = people_df.drop(columns=[\"complexion\"])\n",
    "    return contact_df, people_df\n",
    "\n",
    "\n",
    "@register_transform\n",
    "def drop_deceased_column(contact_df, people_df):\n",
    "    \"\"\"\n",
    "    Drop the `deceased` column from the **people** table, since it doesn't exist in the older system, and since no one \n",
    "    is marked deceased in this dataset.\n",
    "    \"\"\"\n",
    "    people_df = people_df.drop(columns=[\"deceased\"])\n",
    "    return contact_df, people_df\n",
    "\n",
    "\n",
    "@register_transform\n",
    "def reconcile_hair_style(contact_df, people_df):\n",
    "    \"\"\"Reconcile the **people** table's `hair_style` values that seem to mean the same thing.\"\"\"\n",
    "    people_df.hair_style = people_df.hair_style.replace(\n",
    "        {\n",
    "            \"receding / thin\": \"receding or thin\",\n",
    "            \"receding or slightly receding\": \"receding or thin\",\n",
    "            \"bald\": \"bald or balding\",\n",
    "            \"braids\": \"braided\",\n",
    "            \"wig/hair piece\": \"wig or hair piece\",\n",
    "        }\n",
    "    )\n",
    "    return contact_df, people_df\n",
    "\n",
    "\n",
    "@register_transform\n",
    "def reconcile_race(contact_df, people_df):\n",
    "    \"\"\"Reconcile **people** table `race` values that seem to mean the same thing.\"\"\"\n",
    "    people_df.race = people_df.race.replace(\n",
    "        {\"american indian or alaskan native\": \"native american / alaskan native\"}\n",
    "    )\n",
    "    return contact_df, people_df\n",
    "\n",
    "\n",
    "@register_transform\n",
    "def clean_up_state(contact_df, people_df):\n",
    "    \"\"\"Fix recurrent typo in the **contacts** table `state` column.\"\"\"\n",
    "    contact_df.state = contact_df.state.replace({\"MX\": \"MA\"})\n",
    "    return contact_df, people_df\n",
    "\n",
    "\n",
    "@register_transform\n",
    "def reconcile_stop_duration(contact_df, people_df):\n",
    "    \"\"\"Bucket **contacts** table `stop_duration` column values listed as minutes and remove likely typos.\"\"\"\n",
    "\n",
    "    def bucket_stop_duration(d: str) -> str:\n",
    "        try:\n",
    "            d = int(d)\n",
    "        except:\n",
    "            return d\n",
    "\n",
    "        # A stop longer than 8 hours seems really implausible\n",
    "        if d > 500:\n",
    "            return \"null\"\n",
    "        if d < 5:\n",
    "            return \"less than five minutes\"\n",
    "        if d < 10:\n",
    "            return \"five to ten minutes\"\n",
    "        if d < 15:\n",
    "            return \"ten to fifteen minutes\"\n",
    "        if d < 20:\n",
    "            return \"fifteen to twenty minutes\"\n",
    "        if d < 25:\n",
    "            return \"twenty to twenty-five minutes\"\n",
    "        if d < 30:\n",
    "            return \"twenty-five to thirty minutes\"\n",
    "        if d < 45:\n",
    "            return \"thirty to forty-five minutes\"\n",
    "        if d < 60:\n",
    "            return \"forty-five to sixty minutes\"\n",
    "        if d < 120:\n",
    "            return \"one to two hours\"\n",
    "        return \"longer than two hours\"\n",
    "\n",
    "    contact_df.stop_duration = contact_df.stop_duration.apply(bucket_stop_duration)\n",
    "    return contact_df, people_df\n",
    "\n",
    "\n",
    "@register_transform\n",
    "def clean_location_info(contact_df, people_df):\n",
    "    \"\"\"\n",
    "    Combine the **contacts** table's `street` and `streetaddr` columns into single `street` column, \n",
    "    drop `streedaddr`, and trim extra digits from `zip`.\n",
    "    \"\"\"\n",
    "    empty_street = contact_df.street.isnull()\n",
    "    contact_df.loc[empty_street, \"street\"] = contact_df.streetaddr[empty_street]\n",
    "    contact_df.street = contact_df.street.apply(lambda s: s.replace(\"&\", \"/\"))\n",
    "\n",
    "    contact_df = contact_df.drop(columns=[\"streetaddr\"])\n",
    "\n",
    "    contact_df.zip = contact_df.zip.apply(lambda zip: zip.split(\"-\")[0])\n",
    "\n",
    "    return contact_df, people_df\n",
    "\n",
    "\n",
    "@register_transform\n",
    "def clean_empty_values(contact_df, people_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    The values `''`, `'NULL'`, and `'None'` are used to signify no value was entered in various fields across the dataset.\n",
    "    Replace these values with `np.nan`.\n",
    "    \"\"\"\n",
    "    replacements = {\"\": np.nan, \"null\": np.nan, \"none\": np.nan}\n",
    "    contact_df = contact_df.replace(replacements)\n",
    "    people_df = people_df.replace(replacements)\n",
    "    return contact_df, people_df\n",
    "\n",
    "\n",
    "clean_contact_df, clean_people_df = apply_transforms(contact_df, people_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the cleaned data as CSVs and as a nested JSON blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"clean_contact_df.to_csv(\\\"foi_contacts.csv\\\", index=False)\\nclean_people_df.to_csv(\\\"foi_people.csv\\\", index=False)\";\n",
       "                var nbb_formatted_code = \"clean_contact_df.to_csv(\\\"foi_contacts.csv\\\", index=False)\\nclean_people_df.to_csv(\\\"foi_people.csv\\\", index=False)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clean_contact_df.to_csv(\"fio_contacts.csv\", index=False)\n",
    "clean_people_df.to_csv(\"fio_people.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"people_dicts = clean_people_df.groupby(\\\"fc_num\\\").apply(lambda g: g.to_dict(\\\"r\\\"))\";\n",
       "                var nbb_formatted_code = \"people_dicts = clean_people_df.groupby(\\\"fc_num\\\").apply(lambda g: g.to_dict(\\\"r\\\"))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "people_dicts = clean_people_df.groupby(\"fc_num\").apply(lambda g: g.to_dict(\"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 15;\n",
       "                var nbb_unformatted_code = \"json_contact_df = clean_contact_df.set_index(\\\"fc_num\\\")\\njson_contact_df[\\\"people\\\"] = json_contact_df.index.map(\\n    lambda fc_num: people_dicts.loc[fc_num]\\n)\\nwith open(\\\"nested_foi_data.json\\\", \\\"w\\\") as json_file:\\n    json_contact_df.to_json(json_file, orient=\\\"index\\\")\";\n",
       "                var nbb_formatted_code = \"json_contact_df = clean_contact_df.set_index(\\\"fc_num\\\")\\njson_contact_df[\\\"people\\\"] = json_contact_df.index.map(\\n    lambda fc_num: people_dicts.loc[fc_num]\\n)\\nwith open(\\\"nested_foi_data.json\\\", \\\"w\\\") as json_file:\\n    json_contact_df.to_json(json_file, orient=\\\"index\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "json_contact_df = clean_contact_df.set_index(\"fc_num\")\n",
    "json_contact_df[\"people\"] = json_contact_df.index.map(\n",
    "    lambda fc_num: people_dicts.loc[fc_num]\n",
    ")\n",
    "with open(\"nested_fio_data.json\", \"w\") as json_file:\n",
    "    json_contact_df.to_json(json_file, orient=\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto-populate the README with changes applied to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 16;\n",
       "                var nbb_unformatted_code = \"readme_changelist = \\\"\\\\n\\\".join([f\\\"- {f.__doc__.strip()}\\\" for f in df_transforms])\\n\\nwith open(\\\"README.md\\\", \\\"r\\\") as readme:\\n    readme_body = readme.read()\\n    readme_trunc_body, _ = readme_body.split(\\\"## Data-cleaning operations\\\")\\n\\nwith open(\\\"README.md\\\", \\\"w\\\") as readme:\\n    readme.write(\\n        f\\\"{readme_trunc_body}\\\\n## Data-cleaning operations\\\\n{readme_changelist}\\\"\\n    )\";\n",
       "                var nbb_formatted_code = \"readme_changelist = \\\"\\\\n\\\".join([f\\\"- {f.__doc__.strip()}\\\" for f in df_transforms])\\n\\nwith open(\\\"README.md\\\", \\\"r\\\") as readme:\\n    readme_body = readme.read()\\n    readme_trunc_body, _ = readme_body.split(\\\"## Data-cleaning operations\\\")\\n\\nwith open(\\\"README.md\\\", \\\"w\\\") as readme:\\n    readme.write(\\n        f\\\"{readme_trunc_body}\\\\n## Data-cleaning operations\\\\n{readme_changelist}\\\"\\n    )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "readme_changelist = \"\\n\".join([f\"- {f.__doc__.strip()}\" for f in df_transforms])\n",
    "\n",
    "with open(\"README.md\", \"r\") as readme:\n",
    "    readme_body = readme.read()\n",
    "    readme_trunc_body, _ = readme_body.split(\"## Data-cleaning operations\")\n",
    "\n",
    "with open(\"README.md\", \"w\") as readme:\n",
    "    readme.write(\n",
    "        f\"{readme_trunc_body}\\n## Data-cleaning operations\\n{readme_changelist}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
